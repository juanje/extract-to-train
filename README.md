# Extract-to-Train

üöÄ **Educational tool for extracting information from documents and creating high-quality datasets for LLM fine-tuning**

Extract-to-Train is a comprehensive CLI tool that transforms PDF and Markdown documents into training-ready datasets optimized for LoRA/QLoRA fine-tuning. Built with educational transparency, it provides insights into every step of the dataset creation process.

## ‚ú® Features

- üìÑ **Multi-format Document Support**: 
  - **PDF**: Uses [docling](https://github.com/DS4SD/docling) for structure-preserving content extraction
  - **Markdown**: Native parsing with section-aware chunking
- ü§ñ **Local LLM Integration**: Powered by [Ollama](https://ollama.ai/) for complete privacy and control
- üéØ **Multiple Output Formats**: Supports Alpaca, ShareGPT, and OpenAI formats for major fine-tuning frameworks
- üîç **Quality Validation**: Advanced reasoning models ensure dataset quality
- üìä **Educational Insights**: Transparent logging and statistics for learning optimization
- ‚ö° **Performance Optimized**: Efficient processing with intelligent chunking
- üöÄ **Large Document Support**: Progressive saving and chunk limitation for handling extensive documents
- üåê **Multilingual Processing**: Language-aware generation with auto-detection and manual override

## üìÑ Supported Document Formats

| Format | Processing Method | Features |
|--------|------------------|----------|
| **PDF** | [docling](https://github.com/DS4SD/docling) | Structure preservation, OCR support, table extraction |
| **Markdown** | Native parser | Section-aware chunking, metadata extraction, frontmatter support |

### Document Processing Features

- **PDF Documents**:
  - Intelligent text extraction with OCR fallback
  - Table and image content preservation  
  - Page-based chunking with overlap
  - Metadata extraction (title, author, creation date)

- **Markdown Documents**:
  - Section-based intelligent chunking
  - Frontmatter metadata parsing
  - Header hierarchy preservation
  - Clean text extraction with formatting removal

## üéØ Supported Fine-tuning Frameworks

| Format | Compatible Tools | Use Case |
|--------|------------------|----------|
| **Alpaca** | Axolotl, Unsloth, HuggingFace+PEFT | Instruction following |
| **ShareGPT** | Axolotl, Unsloth | Conversational AI |
| **OpenAI** | HuggingFace Transformers+PEFT | API-compatible workflows |

## üöÄ Quick Start

### Prerequisites

1. **Python 3.10+** with `uv` package manager
2. **Ollama** installed and running locally
3. **Required models** downloaded:
   ```bash
   ollama pull llama3.1:8B
   ollama pull deepseek-r1:8B
   ```

### Installation

```bash
# Clone the repository
git clone https://github.com/juanje/extract-to-train.git
cd extract-to-train

# Install with uv
uv sync

# Verify installation
extract-to-train setup
```

### Basic Usage

```bash
# Basic extraction with educational output
extract-to-train extract document.pdf --verbose

# Extract from Markdown documentation
extract-to-train extract technical_guide.md --verbose

# Generate Alpaca format for Axolotl/Unsloth
extract-to-train extract document.pdf --format alpaca --max-pairs 100

# Test with limited chunks from large document
extract-to-train extract large_document.pdf --max-chunks 10 --verbose

# Process with progressive saving and language specification
extract-to-train extract documento.pdf --language es --progress-file progress.jsonl

# Process Markdown with custom models
extract-to-train extract tutorial.md \
    --model-extract mistral:7B \
    --temperature-extract 0.4 \
    --output my_dataset.jsonl

# Skip validation for faster processing
extract-to-train extract document.pdf --skip-validation --max-pairs 50
```

## üìö Educational Features

### Transparent Processing
- **Step-by-step explanations** of each processing phase
- **Model configuration insights** with rationale for settings
- **Quality metrics** and performance statistics
- **Prompt engineering examples** with design explanations

### Learning-Focused Design
- **Verbose mode** (`--verbose`) shows detailed processing information
- **Explain mode** (`--explain`) provides educational context for decisions
- **Configuration explanations** help understand optimal settings
- **Quality analysis** teaches dataset evaluation principles

## üöÄ Large Document Processing

### Progressive Saving
For large documents, Extract-to-Train automatically saves Q&A pairs as they are generated and validated:

```bash
# Progress file is auto-generated by default
extract-to-train extract large_document.pdf --verbose

# Use custom progress file
extract-to-train extract document.pdf --progress-file my_progress.jsonl
```

**Progress file benefits:**
- ‚úÖ **Recovery from interruptions**: No work lost if process stops
- üëÄ **Real-time monitoring**: Review quality during processing  
- üîÑ **Incremental processing**: Resume from where you left off

### Chunk Limitation
Test processing on large documents by limiting chunks:

```bash
# Process only first 5 chunks for quality testing
extract-to-train extract large_manual.pdf --max-chunks 5 --verbose

# Combine with other limits for controlled testing
extract-to-train extract document.pdf --max-chunks 10 --max-pairs 20
```

**Use cases:**
- üìä **Quality assessment**: Test generation quality before full processing
- ‚ö° **Rapid prototyping**: Quick evaluation of document suitability
- üß™ **Parameter tuning**: Test different model configurations efficiently

## üîß Configuration

### Model Selection

The tool uses optimized model configurations based on extensive testing:

```yaml
models:
  extraction:
    name: "llama3.1:8B"           # Superior instruction following
    temperature: 0.3              # Balanced creativity/consistency
  validation:
    name: "deepseek-r1:8B"        # Specialized reasoning model
    temperature: 0.1              # Maximum precision
```

### Processing Options

```yaml
processing:
  chunk_size: 512                # Characters per chunk
  chunk_overlap: 100             # Context preservation
  max_pairs_per_chunk: 5         # Quality over quantity

generation:
  question_types: ["factual", "inferential", "comparative", "analytical"]
  difficulty_levels: ["easy", "medium", "hard"]
  min_answer_length: 50
  max_answer_length: 500
```

### Environment Variables

Customize behavior using environment variables. Copy `env.example` to `.env`:

```bash
# Copy the example configuration
cp env.example .env

# Edit with your preferences
nano .env
```

**Key variables:**
```bash
# Document processing
CHUNK_SIZE=512                    # Chunk size in characters
CHUNK_OVERLAP=100                 # Overlap between chunks

# Model configuration
MODEL_EXTRACT=llama3.1:8B         # Generation model
MODEL_VALIDATE=deepseek-r1:8B     # Validation model
TEMPERATURE_EXTRACT=0.3           # Generation creativity
TEMPERATURE_VALIDATE=0.1          # Validation precision

# Quality control
MIN_CONFIDENCE=0.75               # Minimum Q&A confidence
MAX_PAIRS_PER_CHUNK=3             # Pairs per chunk limit
```

## üìä Output Examples

### Alpaca Format
```jsonl
{"instruction": "What is machine learning according to the document?", "input": "", "output": "Machine learning is a branch of artificial intelligence that enables computers to learn and improve automatically from experience without being explicitly programmed for each specific task."}
```

### ShareGPT Format
```jsonl
{"conversations": [{"from": "human", "value": "According to the document, what advantages do neural networks have?"}, {"from": "gpt", "value": "According to the document, the main advantages of neural networks are: 1) Ability to learn complex non-linear patterns, 2) Adaptability to different data types, 3) Automatic improvement with more training data."}]}
```

### OpenAI Format
```jsonl
{"messages": [{"role": "system", "content": "You are an expert who answers questions based on technical documents."}, {"role": "user", "content": "What does the document say about optimization algorithms?"}, {"role": "assistant", "content": "The document describes several optimization algorithms including SGD, Adam, and RMSprop, explaining that Adam combines the advantages of AdaGrad and RMSprop to achieve faster and more stable convergence."}]}
```

## üõ†Ô∏è Advanced Usage

### Custom System Prompts

```bash
# For conversational models
extract-to-train extract document.pdf \
    --format openai \
    --system-prompt "You are a technical expert specializing in AI concepts."
```

### Quality Control

```bash
# Enable automatic correction of failed pairs
extract-to-train extract document.pdf --auto-correct

# Analyze existing dataset
extract-to-train analyze dataset.jsonl --detailed
```

### Performance Tuning

```bash
# Speed-optimized processing
extract-to-train extract document.pdf \
    --model-extract mistral:7B \
    --model-validate mistral:7B \
    --chunk-size 1500 \
    --skip-validation

# Quality-focused processing
extract-to-train extract document.pdf \
    --temperature-extract 0.2 \
    --temperature-validate 0.05 \
    --chunk-size 800 \
    --chunk-overlap 300
```

## üìà Quality Metrics

The tool provides comprehensive quality analysis:

- **Pass Rate**: Percentage of generated pairs that meet validation criteria
- **Confidence Scores**: AI confidence in each Q&A pair
- **Diversity Metrics**: Distribution across question types and difficulty levels
- **Token Estimation**: For fine-tuning cost calculation
- **Validation Feedback**: Specific issues and improvement suggestions

## üß™ Development

### Running Tests

```bash
# Install development dependencies
uv sync --group dev

# Run tests with coverage
pytest --cov=extract_to_train --cov-report=html

# Run linting
ruff check src/ tests/
ruff format src/ tests/

# Type checking
mypy src/
```

### Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes with tests
4. Ensure code passes linting and type checks
5. Submit a pull request

## üìñ Documentation

- [Getting Started Guide](docs/getting_started.md)
- [Configuration Reference](docs/configuration.md)
- [Best Practices](docs/best_practices.md)
- [API Documentation](docs/api.md)

## üîç Troubleshooting

### Common Issues

**Ollama Connection Failed**
```bash
# Check Ollama status
ollama list

# Start Ollama if needed
ollama serve
```

**Model Not Found**
```bash
# Download required models
ollama pull llama3.1:8B
ollama pull deepseek-r1:8B
```

**Memory Issues with Large PDFs**
```bash
# Reduce chunk size for large documents
extract-to-train extract large_doc.pdf --chunk-size 500
```

## ü§ù Community

- **Issues**: [GitHub Issues](https://github.com/your-username/extract-to-train/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-username/extract-to-train/discussions)
- **Contributing**: See [CONTRIBUTING.md](CONTRIBUTING.md)

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ü§ñ AI Tools Disclaimer

This project was developed with the assistance of artificial intelligence tools:

**Tools used:**
- **Cursor**: Code editor with AI capabilities
- **Claude-4-Sonnet**: Anthropic's language model

**Division of responsibilities:**

**AI (Cursor + Claude-4-Sonnet)**:
- üîß Initial code prototyping
- üìù Generation of examples and test cases
- üêõ Assistance in debugging and error resolution
- üìö Documentation and comments writing
- üí° Technical implementation suggestions

**Human (Juanje Ojeda)**:
- üéØ Specification of objectives and requirements
- üîç Critical review of code and documentation
- üí¨ Iterative feedback and solution refinement
- üìã Definition of project's educational structure
- ‚úÖ Final validation of concepts and approaches

**Collaboration philosophy**: AI tools served as a highly capable technical assistant, while all design decisions, educational objectives, and project directions were defined and validated by the human.

## üôè Acknowledgments

- [docling](https://github.com/DS4SD/docling) for robust PDF processing
- [Ollama](https://ollama.ai/) for local LLM infrastructure
- [LangChain](https://langchain.com/) for LLM orchestration
- The open-source community for inspiration and tools